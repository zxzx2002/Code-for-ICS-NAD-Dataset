{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c2dfa0-0a26-4746-97cc-81811f39b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 7964\n",
      "Training time: 0.0101 seconds\n",
      "Accuracy: 0.9859\n",
      "Class 0 F1 score: 0.8557\n",
      "Class 0 Precision: 0.7477\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9926\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9853\n"
     ]
    }
   ],
   "source": [
    "#ID3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "import time  \n",
    "\n",
    "\n",
    "file_path = 'arpspoof4.0.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].astype('category').cat.codes\n",
    "\n",
    "\n",
    "X = data.drop(columns=['state'])\n",
    "y = data['state']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f'Training data size: {X_train.shape[0]}')\n",
    "\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f'Training time: {training_time:.4f} seconds')\n",
    "\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "precisions = precision_score(y_test, y_pred, average=None)\n",
    "recalls = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "for i, (f1, precision, recall) in enumerate(zip(f1_scores, precisions, recalls)):\n",
    "    print(f'Class {i} F1 score: {f1:.4f}')\n",
    "    print(f'Class {i} Precision: {precision:.4f}')\n",
    "    print(f'Class {i} Recall: {recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2357a39e-5cf6-481d-972a-6c177dc4bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5973\n",
      "Training time: 487.1100 seconds\n",
      "Best parameters found: OrderedDict([('activation', 'ReLU'), ('batch_size', 32), ('epochs', 97), ('hidden_units', 42), ('learning_rate', 0.0070313315344204125)])\n",
      "Best cross-validation accuracy: 1.0000\n",
      "Accuracy on test set: 0.9867\n",
      "Class 0 F1 score: 0.8251\n",
      "Class 0 Precision: 1.0000\n",
      "Class 0 Recall: 0.7022\n",
      "Class 1 F1 score: 0.9931\n",
      "Class 1 Precision: 0.9863\n",
      "Class 1 Recall: 1.0000\n",
      "Model saved as ann_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#ANN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units=64, activation='ReLU'):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, 1)\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_units=64, activation='ReLU', learning_rate=0.001, epochs=50, batch_size=32):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = ANNModel(input_dim, hidden_units, activation)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        X, y = torch.FloatTensor(X), torch.FloatTensor(y).unsqueeze(1)\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            permutation = torch.randperm(X.size()[0])\n",
    "            for i in range(0, X.size()[0], self.batch_size):\n",
    "                indices = permutation[i:i + self.batch_size]\n",
    "                batch_x, batch_y = X[indices], y[indices]\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.FloatTensor(X))\n",
    "            return (outputs > 0.5).numpy().astype(int).ravel()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    y = np.where(y == 1, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv'\n",
    "    test_size = 0.4\n",
    "    random_state = 42\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    param_dist = {\n",
    "        'hidden_units': [32, 64],\n",
    "        'activation': ['ReLU', 'Tanh'],\n",
    "        'learning_rate': [0.001, 0.01],\n",
    "        'epochs': [50, 100],\n",
    "        'batch_size': [32]\n",
    "    }\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        PyTorchClassifier(input_dim=X_train.shape[1]), \n",
    "        search_spaces=param_dist, \n",
    "        n_iter=10, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=1, \n",
    "        cv=3, \n",
    "        random_state=random_state, \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "\n",
    "    print(f\"Training data size: {X_train.shape[0]}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "\n",
    "    best_model = PyTorchClassifier(input_dim=X_train.shape[1], **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy on test set: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "    precisions = precision_score(y_test, y_pred, average=None)\n",
    "    recalls = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "    for i, (f1, precision, recall) in enumerate(zip(f1_scores, precisions, recalls)):\n",
    "        print(f\"Class {i} F1 score: {f1:.4f}\")\n",
    "        print(f\"Class {i} Precision: {precision:.4f}\")\n",
    "        print(f\"Class {i} Recall: {recall:.4f}\")\n",
    "\n",
    "\n",
    "    model_filename = 'ann_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c466ed5c-54ae-452d-b2f0-dd51cfc3598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5973\n",
      "Training time: 3.4380 seconds\n",
      "Best parameters found: OrderedDict([('algorithm', 'ball_tree'), ('leaf_size', 50), ('n_neighbors', 3), ('p', 1), ('weights', 'uniform')])\n",
      "Best cross-validation accuracy: 1.0000\n",
      "Accuracy on test set: 1.0000\n",
      "Class 0 F1 score: 1.0000\n",
      "Class 0 Precision: 1.0000\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 1.0000\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 1.0000\n",
      "Model saved as knn_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import joblib  \n",
    "import time  \n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    # 将目标变量转换为0和1\n",
    "    y = np.where(y == 1, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv'\n",
    "    test_size = 0.4 \n",
    "    random_state = 42 \n",
    "\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': [20, 30, 40, 50],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        KNeighborsClassifier(), \n",
    "        search_spaces=param_dist, \n",
    "        n_iter=10, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=1, \n",
    "        cv=3, \n",
    "        random_state=random_state, \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "\n",
    "    print(f\"Training data size: {X_train.shape[0]}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "\n",
    "\n",
    "    best_model = KNeighborsClassifier(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy on test set: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "    precisions = precision_score(y_test, y_pred, average=None)\n",
    "    recalls = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "    for i, (f1, precision, recall) in enumerate(zip(f1_scores, precisions, recalls)):\n",
    "        print(f\"Class {i} F1 score: {f1:.4f}\")\n",
    "        print(f\"Class {i} Precision: {precision:.4f}\")\n",
    "        print(f\"Class {i} Recall: {recall:.4f}\")\n",
    "\n",
    "\n",
    "    model_filename = 'knn_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4bed00-1dd6-44e8-9c2c-337a615c2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5973\n",
      "Training time: 9.6879 seconds\n",
      "Accuracy: 0.9995\n",
      "Class 0 F1 score: 0.9944\n",
      "Class 0 Precision: 0.9889\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9997\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9995\n",
      "Best parameters: OrderedDict([('bootstrap', True), ('max_depth', 20), ('max_features', 'log2'), ('min_samples_leaf', 1), ('min_samples_split', 10), ('n_estimators', 200)])\n",
      "\n",
      "Features with non-zero importance:\n",
      "start: 0.0006\n",
      "end: 0.0005\n",
      "startOffset: 0.0003\n",
      "endOffset: 0.0008\n",
      "duration: 0.0285\n",
      "sPackets: 0.0050\n",
      "rPackets: 0.0534\n",
      "sBytesSum: 0.0474\n",
      "rBytesSum: 0.0903\n",
      "sBytesMax: 0.0067\n",
      "rBytesMax: 0.0398\n",
      "sBytesMin: 0.0404\n",
      "rBytesMin: 0.0463\n",
      "sBytesAvg: 0.0158\n",
      "rBytesAvg: 0.0383\n",
      "sLoad: 0.0355\n",
      "rLoad: 0.0405\n",
      "sPayloadSum: 0.0047\n",
      "rPayloadSum: 0.0197\n",
      "sPayloadMax: 0.0559\n",
      "rPayloadMax: 0.0202\n",
      "sPayloadMin: 0.0249\n",
      "rPayloadMin: 0.0148\n",
      "sPayloadAvg: 0.0553\n",
      "rPayloadAvg: 0.0242\n",
      "sInterPacketAvg: 0.0035\n",
      "rInterPacketAvg: 0.0207\n",
      "rttl: 0.0131\n",
      "sPshRate: 0.0162\n",
      "sWinTCP: 0.0166\n",
      "rWinTCP: 0.0106\n",
      "sAckDelayMax: 0.0032\n",
      "sAckDelayAvg: 0.0096\n",
      "rAckDelayAvg: 0.0002\n",
      "totalBytes: 0.1698\n",
      "totalPackets: 0.0265\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    return data, X, y\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv'  # CSV文件路径\n",
    "    test_size = 0.4\n",
    "    random_state = 42\n",
    "\n",
    "\n",
    "    data, X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        RandomForestClassifier(random_state=random_state),\n",
    "        search_spaces=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1,\n",
    "        cv=3,\n",
    "        random_state=random_state,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_model = RandomForestClassifier(random_state=random_state, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Training data size: {len(X_train)}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    feature_names = data.columns[:-1]\n",
    "    non_zero_importances = [(name, importance) for name, importance in zip(feature_names, feature_importances) if importance > 0]\n",
    "\n",
    "    print(\"\\nFeatures with non-zero importance:\")\n",
    "    for name, importance in non_zero_importances:\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9feb0a71-c380-4f8b-9693-13a669cd2826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5973\n",
      "Training time: 50.1976 seconds\n",
      "Accuracy: 0.9995\n",
      "Class 0 F1 score: 0.9944\n",
      "Class 0 Precision: 0.9889\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9997\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9995\n",
      "Best parameters: OrderedDict([('learning_rate', 0.1), ('max_depth', 7), ('min_samples_leaf', 4), ('min_samples_split', 2), ('n_estimators', 300), ('subsample', 0.9)])\n",
      "\n",
      "Features with non-zero importance:\n",
      "start: 0.0004\n",
      "end: 0.0003\n",
      "startOffset: 0.0002\n",
      "endOffset: 0.0003\n",
      "duration: 0.0000\n",
      "sPackets: 0.0000\n",
      "rPackets: 0.0000\n",
      "sBytesSum: 0.0000\n",
      "rBytesSum: 0.0000\n",
      "sBytesMax: 0.0000\n",
      "rBytesMax: 0.0000\n",
      "sBytesMin: 0.0001\n",
      "rBytesMin: 0.0000\n",
      "sBytesAvg: 0.0000\n",
      "rBytesAvg: 0.0000\n",
      "sLoad: 0.0003\n",
      "rLoad: 0.0002\n",
      "sPayloadSum: 0.0000\n",
      "rPayloadSum: 0.0000\n",
      "sPayloadMax: 0.0001\n",
      "rPayloadMax: 0.0000\n",
      "sPayloadMin: 0.0002\n",
      "rPayloadMin: 0.0000\n",
      "sPayloadAvg: 0.0002\n",
      "rPayloadAvg: 0.0000\n",
      "sInterPacketAvg: 0.0000\n",
      "rInterPacketAvg: 0.0000\n",
      "totalBytes: 0.9977\n"
     ]
    }
   ],
   "source": [
    "# GBDT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    return data, X, y\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv'  # CSV文件路径\n",
    "    test_size = 0.4\n",
    "    random_state = 42\n",
    "\n",
    "\n",
    "    data, X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        GradientBoostingClassifier(random_state=random_state),\n",
    "        search_spaces=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1,\n",
    "        cv=3,\n",
    "        random_state=random_state,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_model = GradientBoostingClassifier(random_state=random_state, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Training data size: {len(X_train)}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    feature_names = data.columns[:-1]\n",
    "    non_zero_importances = [(name, importance) for name, importance in zip(feature_names, feature_importances) if importance > 0]\n",
    "\n",
    "    print(\"\\nFeatures with non-zero importance:\")\n",
    "    for name, importance in non_zero_importances:\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e978fad-6543-4ff6-99b4-c2dda24791d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5973\n",
      "Training time: 8.6290 seconds\n",
      "Accuracy: 0.9995\n",
      "Class 0 F1 score: 0.9944\n",
      "Class 0 Precision: 0.9889\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9997\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9995\n",
      "Best parameters: OrderedDict([('colsample_bytree', 0.8), ('gamma', 0.2), ('learning_rate', 0.2), ('max_depth', 3), ('n_estimators', 200), ('subsample', 0.8)])\n",
      "\n",
      "Features with non-zero importance:\n",
      "Feature 5: 0.0010\n",
      "Feature 7: 0.0295\n",
      "Feature 8: 0.2541\n",
      "Feature 15: 0.0009\n",
      "Feature 16: 0.0011\n",
      "Feature 19: 0.0020\n",
      "Feature 21: 0.0014\n",
      "Feature 23: 0.0030\n",
      "Feature 38: 0.7070\n",
      "Model saved as xgboost_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv' \n",
    "    test_size = 0.4\n",
    "    random_state = 42\n",
    "\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        search_spaces=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        cv=3,\n",
    "        random_state=random_state,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Training data size: {len(X_train)}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    non_zero_importances = [(i, importance) for i, importance in enumerate(feature_importances) if importance > 0]\n",
    "\n",
    "    print(\"\\nFeatures with non-zero importance:\")\n",
    "    for feature_idx, importance in non_zero_importances:\n",
    "        print(f\"Feature {feature_idx}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "    model_filename = 'xgboost_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f'Model saved as {model_filename}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c29717-dfed-4050-90c1-2921133a5570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cf8a47-ec44-4bb0-a95a-62abc3504da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: OrderedDict([('colsample_bytree', 0.8), ('learning_rate', 0.2), ('max_depth', -1), ('n_estimators', 50), ('num_leaves', 63), ('subsample', 0.8)])\n",
      "Best cross-validation accuracy: 0.9998\n",
      "Training time: 13.1785 seconds\n",
      "Accuracy on test set: 0.9995\n",
      "Class 0 F1 score: 0.9944\n",
      "Class 0 Precision: 0.9889\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9997\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9995\n",
      "\n",
      "Features with non-zero importance:\n",
      "Feature 0: 523.0000\n",
      "Feature 1: 80.0000\n",
      "Feature 2: 23.0000\n",
      "Feature 4: 41.0000\n",
      "Feature 5: 37.0000\n",
      "Feature 6: 31.0000\n",
      "Feature 7: 10.0000\n",
      "Feature 8: 41.0000\n",
      "Feature 9: 6.0000\n",
      "Feature 10: 7.0000\n",
      "Feature 11: 4.0000\n",
      "Feature 13: 22.0000\n",
      "Feature 14: 3.0000\n",
      "Feature 15: 45.0000\n",
      "Feature 16: 26.0000\n",
      "Feature 17: 3.0000\n",
      "Feature 18: 1.0000\n",
      "Feature 19: 8.0000\n",
      "Feature 21: 1.0000\n",
      "Feature 23: 1.0000\n",
      "Feature 25: 13.0000\n",
      "Feature 26: 14.0000\n",
      "Feature 28: 1.0000\n",
      "Feature 29: 8.0000\n",
      "Feature 30: 9.0000\n",
      "Feature 31: 2.0000\n",
      "Feature 34: 10.0000\n",
      "Feature 36: 7.0000\n",
      "Feature 37: 5.0000\n",
      "Feature 38: 86.0000\n",
      "Feature 39: 1.0000\n",
      "Model saved as lightgbm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    y = np.where(y == 1, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv' \n",
    "    test_size = 0.4 \n",
    "    random_state = 42  \n",
    "\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'num_leaves': [15, 31, 63],        \n",
    "        'max_depth': [3, 5, -1],            \n",
    "        'learning_rate': [0.01, 0.1, 0.2],  \n",
    "        'n_estimators': [50, 100, 200],     \n",
    "        'subsample': [0.6, 0.8, 1.0],       \n",
    "        'colsample_bytree': [0.6, 0.8, 1.0] \n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        LGBMClassifier(verbose=-1), \n",
    "        search_spaces=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        cv=3,\n",
    "        random_state=random_state,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "    best_model = LGBMClassifier(verbose=-1, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on test set: {test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    "\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    non_zero_importances = [(i, importance) for i, importance in enumerate(feature_importances) if importance > 0]\n",
    "\n",
    "    print(\"\\nFeatures with non-zero importance:\")\n",
    "    for feature_idx, importance in non_zero_importances:\n",
    "        print(f\"Feature {feature_idx}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "    model_filename = 'lightgbm_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f'Model saved as {model_filename}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7785e4d0-5e28-4b21-bf70-c3e16e0ae54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: OrderedDict([('C', 1), ('gamma', 'auto'), ('kernel', 'rbf')])\n",
      "Best cross-validation accuracy: 1.0000\n",
      "Training time: 1.1885 seconds\n",
      "Accuracy on test set: 1.0000\n",
      "Class 0 F1 score: 1.0000\n",
      "Class 0 Precision: 1.0000\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 1.0000\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 1.0000\n",
      "Model saved as svc_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from skopt import BayesSearchCV\n",
    "import joblib  \n",
    "import time \n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    y = np.where(y == 1, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv' \n",
    "    test_size = 0.4   \n",
    "    random_state = 42  \n",
    "\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        SVC(), \n",
    "        search_spaces=param_dist, \n",
    "        n_iter=10, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=1, \n",
    "        cv=3, \n",
    "        random_state=random_state, \n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")  \n",
    "\n",
    "  \n",
    "    best_model = SVC(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    " \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on test set: {test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, average=None, labels=[cls])[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    " \n",
    "    model_filename = 'svc_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f'Model saved as {model_filename}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fc1978-9b55-4776-bc1c-c8802485593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: OrderedDict([('learning_rate', 0.1), ('n_estimators', 200)])\n",
      "Best cross-validation accuracy: 0.9998\n",
      "Training time: 28.8580 seconds\n",
      "Accuracy on test set: 0.9995\n",
      "Class 0 F1 score: 0.9944\n",
      "Class 0 Precision: 0.9889\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9997\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9995\n",
      "Model saved as adaboost_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#adaboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    y = np.where(y == 1, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv'  \n",
    "    test_size = 0.4  \n",
    "    random_state = 42 \n",
    "\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1]\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        AdaBoostClassifier(), \n",
    "        search_spaces=param_dist, \n",
    "        n_iter=10, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=1, \n",
    "        cv=3, \n",
    "        random_state=random_state, \n",
    "        verbose=0  \n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "    best_model = AdaBoostClassifier(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on test set: {test_accuracy:.4f}')\n",
    "\n",
    " \n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, labels=[cls], average=None)[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, labels=[cls], average=None)[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, labels=[cls], average=None)[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    "\n",
    "    model_filename = 'adaboost_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f'Model saved as {model_filename}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5fde88-eb13-464c-82dd-80ac127ab56a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: OrderedDict([('activation', 'tanh'), ('hidden_units', 200)])\n",
      "Best cross-validation accuracy: 0.9993\n",
      "Training time: 1.6369 seconds\n",
      "Accuracy on test set: 0.9992\n",
      "Class 0 F1 score: 0.9916\n",
      "Class 0 Precision: 0.9834\n",
      "Class 0 Recall: 1.0000\n",
      "Class 1 F1 score: 0.9996\n",
      "Class 1 Precision: 1.0000\n",
      "Class 1 Recall: 0.9992\n",
      "Model saved as elm_model.h5\n"
     ]
    }
   ],
   "source": [
    "#elm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from skopt import BayesSearchCV\n",
    "import h5py\n",
    "import time\n",
    "import warnings\n",
    "import hpelm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class SuppressOutput:\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        self._stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stderr.close()\n",
    "        sys.stdout = self._stdout\n",
    "        sys.stderr = self._stderr\n",
    "\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    y = np.where(y == 1, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class ELMClassifier:\n",
    "    def __init__(self, input_dim, hidden_units=100, activation='sigm'):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        with SuppressOutput():\n",
    "            self.model = hpelm.ELM(input_dim, 2)  \n",
    "            self.model.add_neurons(hidden_units, activation)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_one_hot = np.zeros((y.size, y.max() + 1))\n",
    "        y_one_hot[np.arange(y.size), y] = 1\n",
    "        with SuppressOutput():\n",
    "            self.model.train(X, y_one_hot, 'c')\n",
    "\n",
    "    def predict(self, X):\n",
    "        with SuppressOutput():\n",
    "            pred = self.model.predict(X)\n",
    "        return np.argmax(pred, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"input_dim\": self.input_dim, \"hidden_units\": self.hidden_units, \"activation\": self.activation}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        with SuppressOutput():\n",
    "            self.model = hpelm.ELM(self.input_dim, 2)  \n",
    "            self.model.add_neurons(self.hidden_units, self.activation)\n",
    "        return self\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_file_path = 'arpspoof4.0.csv'  \n",
    "    test_size = 0.4  \n",
    "    random_state = 42 \n",
    "\n",
    "\n",
    "    X, y = load_data(csv_file_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    param_dist = {\n",
    "        'hidden_units': [50, 100, 200],\n",
    "        'activation': ['sigm', 'tanh']\n",
    "    }\n",
    "\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        ELMClassifier(input_dim=X_train.shape[1]), \n",
    "        search_spaces=param_dist, \n",
    "        n_iter=10, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=1, \n",
    "        cv=3, \n",
    "        random_state=random_state, \n",
    "        verbose=0 \n",
    "    )\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "    print(f\"Training time: {training_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "    best_model = ELMClassifier(input_dim=X_train.shape[1], **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on test set: {test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "    unique_classes = np.unique(y_test)\n",
    "    for cls in unique_classes:\n",
    "        cls_f1 = f1_score(y_test, y_pred, labels=[cls], average=None)[0]\n",
    "        cls_precision = precision_score(y_test, y_pred, labels=[cls], average=None)[0]\n",
    "        cls_recall = recall_score(y_test, y_pred, labels=[cls], average=None)[0]\n",
    "        print(f\"Class {cls} F1 score: {cls_f1:.4f}\")\n",
    "        print(f\"Class {cls} Precision: {cls_precision:.4f}\")\n",
    "        print(f\"Class {cls} Recall: {cls_recall:.4f}\")\n",
    "\n",
    "\n",
    "    model_filename = 'elm_model.h5'\n",
    "    with h5py.File(model_filename, 'w') as h5f:\n",
    "        h5f.create_dataset('scaler_mean', data=scaler.mean_)\n",
    "        h5f.create_dataset('scaler_scale', data=scaler.scale_)\n",
    "        h5f.attrs['best_params'] = str(best_params)\n",
    "    print(f'Model saved as {model_filename}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a2ca7-7e9d-4c8c-a531-4c219a77bb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e812aa9-57fe-4e5d-907d-8d6f13d957d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36065f97-f677-429c-892f-87631763ef24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
